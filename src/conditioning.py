# https://github.com/Extraltodeus/Vector_Sculptor_ComfyUI
import torch
import comfy.model_management as model_management
from copy import deepcopy
from .bnk_adv_encode import advanced_encode
def maximum_absolute_values(tensors,reversed=False):
    shape = tensors.shape
    tensors = tensors.reshape(shape[0], -1)
    tensors_abs = torch.abs(tensors)
    if not reversed:
        max_abs_idx = torch.argmax(tensors_abs, dim=0)
    else:
        max_abs_idx = torch.argmin(tensors_abs, dim=0)
    result = tensors[max_abs_idx, torch.arange(tensors.shape[1])]
    return result.reshape(shape[1:])

def get_closest_token_cosine_similarities(single_weight, all_weights, return_scores=False):
    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
    scores = cos(all_weights, single_weight.unsqueeze(0).to(all_weights.device))
    sorted_scores, sorted_ids = torch.sort(scores, descending=True)
    best_id_list = sorted_ids.tolist()
    if not return_scores:
        return best_id_list
    scores_list = sorted_scores.tolist()
    return best_id_list, scores_list

def get_single_cosine_score(single_weight,concurrent_weight):
    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
    score = cos(concurrent_weight.unsqueeze(0), single_weight.unsqueeze(0)).item()
    return score

def refine_token_weight(token_id, all_weights, sculptor_method, sculptor_multiplier):
    initial_weight = all_weights[token_id]
    pre_mag = torch.norm(initial_weight)
    concurrent_weights_ids, scores = get_closest_token_cosine_similarities(initial_weight,all_weights,True)
    concurrent_weights_ids, scores = concurrent_weights_ids[1:], scores[1:]
    
    previous_cos_score = 0
    cos_score = 1
    iter_num = 0
    s = []
    tmp_weights = []
    ini_w = torch.clone(initial_weight)

    while previous_cos_score < cos_score:
        if iter_num > 0:
            previous_cos_score = cos_score
        s.append(scores[iter_num])
        current_weight = all_weights[concurrent_weights_ids[iter_num]]
        tmp_weights.append(current_weight)
        vec_sum = torch.sum(torch.stack(tmp_weights),dim=0)
        cos_score = get_single_cosine_score(ini_w, vec_sum)
        iter_num += 1
    del s[-1]
    del tmp_weights[-1]

    if len(s) <= 1: return initial_weight.cpu(), 0

    if sculptor_method == "maximum_absolute":
        concurrent_weights = torch.stack([ini_w/torch.norm(ini_w)]+[t/torch.norm(t) for i, t in enumerate(tmp_weights)])
        initial_weight = maximum_absolute_values(concurrent_weights)
        initial_weight *= pre_mag / torch.norm(initial_weight)
        return initial_weight.cpu(), len(s)

    concurrent_weights = torch.sum(torch.stack([t * s[i]**2 for i, t in enumerate(tmp_weights)]), dim=0)
    final_score = get_single_cosine_score(initial_weight,concurrent_weights) * sculptor_multiplier

    if sculptor_method == "backward":
        initial_weight = initial_weight + concurrent_weights * final_score
    elif sculptor_method == "forward":
        initial_weight = initial_weight - concurrent_weights * final_score
        
    initial_weight *= pre_mag / torch.norm(initial_weight)
    return initial_weight.cpu(), len(s)

def vector_sculptor_tokens(clip, text, sculptor_method, token_normalization, sculptor_multiplier):
    ignored_token_ids = [49406, 49407, 0]
    initial_tokens = clip.tokenize(text)
    total_found = 0
    total_replaced = 0
    total_candidates = 0
    
    for k in initial_tokens:
        mean_mag = 0
        mean_mag_count = 0
        to_mean_coords = []
        clip_model = getattr(clip.cond_stage_model, f"clip_{k}", None)
        all_weights = torch.clone(clip_model.transformer.text_model.embeddings.token_embedding.weight).to(device=model_management.get_torch_device())
        if token_normalization == "mean of all tokens":
            all_mags = torch.stack([torch.norm(t) for t in all_weights])
            mean_mag_all_weights = torch.mean(all_mags, dim=0).item()
        for x in range(len(initial_tokens[k])):
            for y in range(len(initial_tokens[k][x])):
                token_id, attn_weight = initial_tokens[k][x][y]
                if token_id not in ignored_token_ids and sculptor_multiplier > 0:
                    total_candidates += 1
                    new_vector, n_found = refine_token_weight(token_id,all_weights, sculptor_method, sculptor_multiplier)
                    if n_found > 0:
                        total_found += n_found
                        total_replaced += 1
                else:
                    new_vector = all_weights[token_id]
                # if y not in [0,76] and token_normalization != "none":
                if token_normalization != "none":
                    if token_normalization == "mean" or token_normalization == "mean * attention":
                        mean_mag += torch.norm(new_vector).item()
                        mean_mag_count += 1
                        to_mean_coords.append([x,y])
                    elif token_normalization == "set at 1":
                        new_vector /= torch.norm(new_vector)
                    elif token_normalization == "default * attention":
                        new_vector *= attn_weight
                    elif token_normalization == "set at attention":
                        new_vector /= torch.norm(new_vector) * attn_weight
                    elif token_normalization == "mean of all tokens":
                        new_vector /= torch.norm(new_vector) * mean_mag_all_weights
                initial_tokens[k][x][y] = (new_vector, attn_weight)
        if (token_normalization == "mean" or token_normalization == "mean * attention") and mean_mag_count > 0:
            mean_mag /= mean_mag_count
            for x, y in to_mean_coords:
                token_weight, attn_weight = initial_tokens[k][x][y]
                if token_normalization == "mean * attention":
                    twm = attn_weight
                else:
                    twm = 1
                token_weight = token_weight / torch.norm(token_weight) * mean_mag * twm
                initial_tokens[k][x][y] = (token_weight, attn_weight)
        del all_weights
    if total_candidates > 0:
        print(f"total_found: {total_found} / total_replaced: {total_replaced} / total_candidates: {total_candidates} / candidate proportion replaced: {round(100*total_replaced/total_candidates,2)}%")
    return initial_tokens



def add_to_first_if_shorter(conditioning1,conditioning2,x=0):
    min_dim = min(conditioning1[x][0].shape[1],conditioning2[x][0].shape[1])
    if conditioning2[x][0].shape[1]>conditioning1[x][0].shape[1]:
        conditioning2[x][0][:,:min_dim,...] = conditioning1[x][0][:,:min_dim,...]
        conditioning1 = conditioning2
    return conditioning1

# cheap slerp / I will bet an eternity doing regex that this is the dark souls 2 camera direction formula
def average_and_keep_mag(v1,v2,p1):
    m1 = torch.norm(v1)
    m2 = torch.norm(v2)
    v0 = v1 * p1 + v2 * (1 - p1)
    v0 = v0 / torch.norm(v0) * (m1 * p1 + m2 * (1 - p1))
    return v0

# from  https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475
def slerp(high, low, val):
    dims = low.shape

    #flatten to batches
    low = low.reshape(dims[0], -1)
    high = high.reshape(dims[0], -1)

    low_norm = low/torch.norm(low, dim=1, keepdim=True)
    high_norm = high/torch.norm(high, dim=1, keepdim=True)

    # in case we divide by zero
    low_norm[low_norm != low_norm] = 0.0
    high_norm[high_norm != high_norm] = 0.0

    omega = torch.acos((low_norm*high_norm).sum(1))
    so = torch.sin(omega)
    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high
    return res.reshape(dims)
    
class PromptConditioning:
    """
    sculptor_intensity:æ•ˆæœçš„å¼ºåº¦ã€‚å¦‚æœæ–¹å‘æ²¡æœ‰åè½¬ï¼Œæœ€å¤šåˆ°3ï¼Œä½ å°†ä¿ç•™ä½ æç¤ºçš„æ€»ä½“æ„ä¹‰ã€‚å¦‚æœåè½¬ï¼Œåˆ™ä¸è¦è¶…è¿‡1~2.5ï¼Œå¦åˆ™éšæœºæ€§å¢å¼ºã€‚
    sculptor_method:
        forwardï¼šå‡å»æœ€è¿‘çš„å‘é‡ã€‚è¶…è¿‡1å¯èƒ½ä¼šæœ‰é€†æ•ˆæœã€‚
        backwardï¼šç›¸åï¼ŒåŠ ä¸Šå®ƒä»¬ã€‚
        maximum_absoluteï¼šè§„èŒƒåŒ–å‘é‡å¹¶é€‰æ‹©æœ€è¿œç¦»0çš„å€¼ã€‚é™¤äº†è®¾ç½®ä¸º0æ—¶ç¦ç”¨å¤–ï¼Œå¼ºåº¦åœ¨è¿™é‡Œæ²¡æœ‰æ•ˆæœã€‚è¿™å€¾å‘äºä½¿ç®€å•ä¸»é¢˜çš„æ„æˆæ›´å¤æ‚ï¼Œå¤æ‚æç¤ºæ›´æ··ä¹±ã€‚æ ¹æ®æƒ…å†µå¯èƒ½æœ‰ç›Šä¹Ÿå¯èƒ½æ²¡æœ‰ã€‚å®ƒä¸»è¦æ˜¯ä¸ºäº†å¥½ç©ï¼Œä½†å¯¹äºæŠ½è±¡æ¦‚å¿µå¯ä»¥äº§ç”Ÿæå¥½çš„ç»“æœã€‚
    token_normalizationï¼šé‡æ–°å·¥ä½œæ¯ä¸ªå‘é‡çš„å¤§å°ã€‚
    å»ºè®®è¦ä¹ˆâ€œæ— â€ä¿ç•™é»˜è®¤è®¾ç½®ï¼Œè¦ä¹ˆâ€œå¹³å‡â€è®¾ç½®æ¯ä¸ªä»¤ç‰Œçš„é‡è¦æ€§ä¸ºå®ƒä»¬çš„æ•´ä½“å¹³å‡å€¼ã€‚
    â€œè®¾ç½®ä¸º1â€ä¼šå°†å®ƒä»¬å…¨éƒ¨è®¾ç½®ä¸º1ï¼Œæˆ‘ä¸çŸ¥é“è¿™æ˜¯å¦æ˜¯ä¸ªå¥½ä¸»æ„ã€‚
    â€œæ‰€æœ‰ä»¤ç‰Œçš„å¹³å‡å€¼â€å°†å–é¢„è®¾æ¡ä»¶æƒé‡å†…æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼ï¼Œè¿™å¯èƒ½æ˜¯ä¸ªåä¸»æ„ï¼Œä½†ä¹Ÿä¸ºä»€ä¹ˆä¸å‘¢ã€‚
    å¦‚æœå¼ºåº¦è®¾ç½®ä¸º0ï¼Œä»¤ç‰Œçš„è§„èŒƒåŒ–ä»ç„¶æœ‰æ•ˆã€‚å°†å…¶è®¾ç½®ä¸º0å¹¶é€‰æ‹©â€œæ— â€å°†è¿”å›é»˜è®¤çš„èˆ’é€‚æ¡ä»¶ã€‚

    æ— è®ºä¸»é¢˜å¦‚ä½•ï¼Œä¸¤ä¸ªæ–¹å‘éƒ½æä¾›æœ‰æ•ˆçš„å˜åŒ–ã€‚

    å¯¹äºä¸€èˆ¬ç”¨é€”ï¼Œæˆ‘æ¨èå‘åè®¾ç½®0.5ç”¨äºæ­£é¢æç¤ºï¼Œå¹¶å¯¹äºè´Ÿé¢æç¤ºâ€œä¿æŒåŸä½â€ã€‚

    å°†ä»¤ç‰Œå¤§å°è§„èŒƒåŒ–ä¸ºå®ƒä»¬çš„å¹³å‡å€¼ä¼¼ä¹ä¹Ÿæœ‰ç§¯æçš„æ•ˆæœã€‚ç‰¹åˆ«æ˜¯å¯¹äºè´Ÿé¢æç¤ºï¼Œè¿™ä¼¼ä¹é™ä½äº†æ¯åå›¾åƒçš„æ¯”ä¾‹ã€‚
    merge_conditioning_typeï¼šçƒé¢çº¿æ€§æ’å€¼ï¼Œå¹³å‡
    merge_conditioning_strength_customï¼š â€œåˆ†åˆ«è®¾ç½®\\n\\næ¢è¡Œçš„promptçš„å€¼ï¼Œç”¨é€—å·åˆ†å‰²ï¼Œæ•°é‡ä¸ºåˆ†æ®µæ•°é‡-1â€
    """
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "clip": ("CLIP", ),
                "text": ("STRING", {"multiline": True}),
                "merge_conditioning_type":(["slerp","average"], {"default": "average"}),
                "merge_conditioning_strength": ("FLOAT", {"default": 0.5, "min": 0, "max": 1, "step": 0.01}),
                "merge_conditioning_strength_custom": ("STRING", {"multiline": True} ),
                "sculptor_intensity": ("FLOAT", {"default": 1, "min": 0, "max": 10, "step": 0.1}),
                "sculptor_method" : (["forward","backward","maximum_absolute"],{"default":"backward"}),
                "token_normalization": (["none", "mean", "set at 1", "default * attention", "mean * attention", "set at attention", "mean of all tokens"],),
 
            }
        }

    FUNCTION = "exec"
    RETURN_TYPES = ("CONDITIONING",)
    CATEGORY = "fofoğŸ¼/conditioning"

    def exec(
            self, 
            clip, 
            text,
            merge_conditioning_type, 
            merge_conditioning_strength,
            merge_conditioning_strength_custom, 
            sculptor_method, 
            token_normalization, 
            sculptor_intensity,
            ):
        cond_list = []
        prompt_list = []

        for line in text.split("\n\n"):
            line = line.strip()
            if len(line) == 0:
                continue
            prompt_list.append(line)

        if merge_conditioning_strength_custom is not None and merge_conditioning_strength_custom != "":
            merge_conditioning_strength = [float(x) for x in merge_conditioning_strength_custom.split(",")]
            if len(merge_conditioning_strength) != len(prompt_list) - 1:
                raise ValueError(f"merge_conditioning_strength_custom should have {len(prompt_list) - 1} values")
        else:
            merge_conditioning_strength=[merge_conditioning_strength] * (len(prompt_list) - 1)
        print(f"merge_conditioning_strength: {merge_conditioning_strength}")
        
        for line in prompt_list:

            sculptor_tokens = vector_sculptor_tokens(clip, line, sculptor_method, token_normalization, sculptor_intensity)
            cond, pooled = clip.encode_from_tokens(sculptor_tokens, return_pooled=True)
            conditioning = [[cond, {"pooled_output": pooled}]]
            cond_list.append(conditioning)

        if len(cond_list) == 1:
            return (cond_list[0],)
        

        
        
        if merge_conditioning_type == "slerp":
            merge_func = slerp
        else:
            merge_func = average_and_keep_mag
            merge_conditioning_strength=[x*2 for x in merge_conditioning_strength]
        
        cond1 = deepcopy(cond_list[0])
        for i in range(1, len(cond_list)):
            cond2 = deepcopy(cond_list[i])  # å½“å‰å¾ªç¯çš„ cond2 ä¸º cond_list çš„ç¬¬iå·å…ƒç´ 
            # è¿›è¡ŒåŸå…ˆçš„å¤„ç†é€»è¾‘
            for x in range(min(len(cond1), len(cond2))):
                min_dim = min(cond1[x][0].shape[1], cond2[x][0].shape[1])
                if cond1[x][0].shape[2] == 2048:
                    cond1[x][0][:, :min_dim, :768] = merge_func(cond1[x][0][:, :min_dim, :768], cond2[x][0][:, :min_dim, :768], merge_conditioning_strength[i-1])
                    cond1[x][0][:, :min_dim, 768:] = merge_func(cond1[x][0][:, :min_dim, 768:], cond2[x][0][:, :min_dim, 768:], merge_conditioning_strength[i-1])
                else:
                    cond1[x][0][:, :min_dim, ...] = merge_func(cond1[x][0][:, :min_dim, ...], cond2[x][0][:, :min_dim, ...], merge_conditioning_strength[i-1])
                cond1 = add_to_first_if_shorter(cond1, cond2, x)
        return (cond1,)

class AdvancedCLIPTextEncode:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "text": ("STRING", {"multiline": True}),
            "clip": ("CLIP",),
            "token_normalization": (["none", "mean", "length", "length+mean"],),
            "weight_interpretation": (["comfy", "A1111", "compel", "comfy++", "down_weight"],),
            # "affect_pooled": (["disable", "enable"],),
        }}

    RETURN_TYPES = ("CONDITIONING",)
    FUNCTION = "encode"

    CATEGORY = "fofoğŸ¼/conditioning"

    def encode(self, clip, text, token_normalization, weight_interpretation, affect_pooled='disable'):
        embeddings_final, pooled = advanced_encode(clip, text, token_normalization, weight_interpretation, w_max=1.0,
                                                   apply_to_pooled=affect_pooled == 'enable')
        return ([[embeddings_final, {"pooled_output": pooled}]],)
